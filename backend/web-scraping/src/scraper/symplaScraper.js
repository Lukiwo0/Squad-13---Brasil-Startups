import puppeteer from 'puppeteer'

export async function getEventLinks(baseUrl, limitLinks = 100, limitPages = 20) {
  const browser = await puppeteer.launch({ headless: true })
  const page = await browser.newPage()

  await page.goto(baseUrl, { waitUntil: 'networkidle2', timeout: 60000 })

  const links = []
  let currentPage = 1

  while (links.length < limitLinks && currentPage <= limitPages) {
    // Espera at√© que os links apare√ßam na p√°gina atual
    try {
      await page.waitForSelector('li.ais-Hits-item a.sympla-card', { timeout: 60000 })
    } catch {
      console.warn(`‚ö†Ô∏è Timeout esperando links na p√°gina ${currentPage}`)
      break
    }

    // Coleta todos os links vis√≠veis da p√°gina
    const newLinks = await page.$$eval('li.ais-Hits-item a.sympla-card', els =>
      els.map(el => el.href).filter(h => h.startsWith('https://'))
    )

    newLinks.forEach(link => {
      if (!links.includes(link)) links.push(link)
    })

    if (links.length >= limitLinks) break

    // Clica no bot√£o "Pr√≥ximo" se existir
    const hasNext = await page.evaluate(() => {
      const btns = Array.from(document.querySelectorAll('button'))
      const nextBtn = btns.find(b => b.textContent.includes('Pr√≥ximo'))
      if (nextBtn && !nextBtn.disabled) {
        nextBtn.click()
        return true
      }
      return false
    })

    if (!hasNext) break

    // Espera o carregamento da nova p√°gina (JS do site renderizando)
    await new Promise(resolve => setTimeout(resolve, 2500))

    currentPage++
  }

  await browser.close()
  return links.slice(0, limitLinks)
}



export async function getEventData(url) {
  const browser = await puppeteer.launch({
    headless: true,
    args: ['--no-sandbox', '--disable-setuid-sandbox'],
  })
  const pageP = await browser.newPage()

  await pageP.goto(url, { waitUntil: 'domcontentloaded', timeout: 60000 })
  await pageP.waitForSelector('#__NEXT_DATA__', { timeout: 60000 })

  const scriptData = await pageP.$eval('#__NEXT_DATA__', el => el.textContent)
  await browser.close()

  if (!scriptData) return null

  const json = JSON.parse(scriptData)
  const event = json.props?.pageProps?.hydrationData?.eventHydration?.event
  if (!event) return null

  return {
    id: event.id,
    name: event.name,
    newUrl: event.newUrl,
    startDate: event.startDate,
    endDate: event.endDate,
    eventsCategory: event.eventsCategory?.name || null,
    eventsHost: event.eventsHost?.name || null,
    paymentEventType: event.paymentEventType || null,
    cancelled: event.cancelled ?? false,
    strippedDetail: event.strippedDetail,
    eventsAddress: event.eventsAddress
      ? {
        city: event.eventsAddress.city || null,
        state: event.eventsAddress.state || null,
        country: event.eventsAddress.country || null,
      }
      : null,
  }
}



export async function scrapeWithLimit(baseUrl, limitLinks, limitPages) {
  const links = await getEventLinks(baseUrl, limitLinks, limitPages)
  const events = []

  console.log('\n')
  console.log('‚ö†Ô∏è LIMITE DE LINK(S) que o scraper ir√° coletar:', limitLinks)
  console.log('‚ö†Ô∏è LIMITE DE P√ÅGINA(S) que o scraper ir√° coletar:', limitPages)
  console.log('\n')

  console.log('üîó Links coletados:', links)
  console.log('\n')

  for (const [index, link] of links.entries()) {
    try {
      console.log(`üïµÔ∏è‚Äç‚ôÇÔ∏è Coletando evento ${index + 1}/${links.length}...`)
      const data = await getEventData(link)
      if (data) events.push(data)
    } catch (err) {
      console.error(`‚ö†Ô∏è Erro ao coletar do evento evento ${index + 1}/${links.length}, Link: ${link}:`, err.message)
    }
  }

  return events
}
